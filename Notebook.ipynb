{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(): \n",
    "    def __init__(self, ):\n",
    "        self.mean_x = None \n",
    "        self.std_x = None \n",
    "\n",
    "    def fit(self, X): \n",
    "        self.mean_x = np.mean(X, axis= 0, keepdims= True) \n",
    "        self.std_x = np.std(X, axis= 0, keepdims= True)  + 1e-12 # Ensure not devision byy zero \n",
    "\n",
    "    def transform(self, X): \n",
    "        X_norm = ( X - self.mean_x )  / self.std_x \n",
    "        return X_norm\n",
    "\n",
    "    def inverse_transform(self, X_norm): \n",
    "        X = X_norm * self.std_x + self.mean_x \n",
    "        return X \n",
    "    \n",
    "    def fit_transform(self, X): \n",
    "        self.fit(X) \n",
    "        return self.transform(X)\n",
    "\n",
    "class OnehotEncoder():\n",
    "    def __init__(self):\n",
    "        self.unique_classes = None \n",
    "        self.labels = None \n",
    "        self.encoded_labels = None \n",
    "\n",
    "    def fit(self, labels):\n",
    "        self.labels = labels # Save labels \n",
    "\n",
    "        # Unique class in this labels \n",
    "        self.unique_classes = np.unique(self.labels) \n",
    "        print(f'Unique classes: {self.unique_classes}')\n",
    "\n",
    "    def transform(self): \n",
    "        # Convert unique_class to index (int number) \n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(self.unique_classes)}\n",
    "\n",
    "        # Create indices array ( map class -> int )\n",
    "        indices = [class_to_index[label] for label in self.labels]\n",
    "\n",
    "        # Encoded matrix \n",
    "        self.encoded_labels = np.zeros(shape= (len(self.labels), len(self.unique_classes)))\n",
    "        self.encoded_labels[np.arange(len(self.labels)), indices] = 1 \n",
    "\n",
    "        return self.encoded_labels \n",
    "    \n",
    "    def inverse_transform(self,y_onehot ): \n",
    "        # Convert idx -> class \n",
    "        index_to_class = {idx : cls for idx, cls in enumerate(self.unique_classes) }\n",
    "\n",
    "        # Create indices array ( map idx -> class ) \n",
    "        indices = np.argmax(y_onehot, axis= 1) \n",
    "        \n",
    "        # Original matrix \n",
    "        original_labels = [index_to_class[idx] for idx in indices ] \n",
    "\n",
    "        return np.array(original_labels)\n",
    "class data:\n",
    "    def __init__(self): \n",
    "        pass \n",
    "\n",
    "    def DataLoader(inputs: np.ndarray, labels: np.ndarray, batchsize: int,random_state: int= None, shuffle= False,): \n",
    "        \"\"\"\n",
    "        This function use to split data to batch \n",
    "        \"\"\"\n",
    "        # Combine inputs, labels -> data \n",
    "        data = list(zip(inputs, labels))\n",
    "\n",
    "        # random.shuffle data \n",
    "        if shuffle: \n",
    "            if isinstance(random_state, int): \n",
    "                np.random.seed(random_state)\n",
    "\n",
    "            np.random.shuffle(data) \n",
    "\n",
    "        # Create batch of data \n",
    "        batches = [data[i: i + batchsize] for i in range(0, len(data), batchsize)] # list \n",
    "\n",
    "        # Convert to np.array \n",
    "        dataloader = [(np.array([item[0] for item in batch]), np.array([item[1] for item in batch])) for batch in batches]\n",
    "\n",
    "        return dataloader  \n",
    "    \n",
    "    def train_test_split(inputs: np.ndarray, labels: np.ndarray,ratio: float, strategy: bool = True, shuffle= True, random_state: int = None):\n",
    "        \"\"\"\n",
    "        This function use to split data to train_dataset, test_dataset\n",
    "        Parameters: \n",
    "            \n",
    "        \"\"\"\n",
    "        # Combine data \n",
    "        data = list(zip(inputs, labels))\n",
    "\n",
    "        # Shulffe data \n",
    "        if shuffle:\n",
    "            if isinstance(random_state, int): \n",
    "                np.random.seed(random_state)\n",
    "        \n",
    "            np.random.shuffle(data) \n",
    "        \n",
    "        if strategy: \n",
    "            train_dataset = [] \n",
    "            test_dataset = []\n",
    "            unique_classed = np.unique(labels) \n",
    "\n",
    "            for cls in unique_classed: \n",
    "                # Get all data of one class at time \n",
    "                current_data = [item for item in data if item[-1] == cls] \n",
    "\n",
    "                # Split data \n",
    "                split_idex = int(ratio * len(current_data)) \n",
    "                train_dataset.extend(current_data[: split_idex])\n",
    "                test_dataset.extend(current_data[split_idex:])\n",
    "        else: \n",
    "            split_idex = int(ratio * len(data))\n",
    "            train_dataset = data[:split_idex] \n",
    "            test_dataset = data[split_idex : ] \n",
    "    \n",
    "        X_train, y_train = zip(*train_dataset)\n",
    "        X_test, y_test = zip(*test_dataset)\n",
    "        \n",
    "        return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __init__(self): \n",
    "        self.grads = {} # Save gradient \n",
    "        self.params = {} # Save weight, bias \n",
    "\n",
    "\n",
    "    #TODO: take forward for neuron network \n",
    "    def forward(self, *inputs): \n",
    "        raise NotImplementedError('Forward method must be implemmented by subclass')\n",
    "\n",
    "    #TODO: compute gradint \n",
    "    def backward(self, *dout):  \n",
    "        raise NotImplementedError('Backward method must implemented by subclasses') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(): \n",
    "    def __init__(self, layers): \n",
    "        self.layers = layers \n",
    "\n",
    "    def forward(self,x):\n",
    "        self.x  = x      # Save input array \n",
    "        for layer in self.layers: \n",
    "           x = layer.forward(x) \n",
    "\n",
    "        return x \n",
    "\n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.layers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        \"\"\"\n",
    "        Initializes a linear (fully connected) layer.\n",
    "\n",
    "        Constraints:\n",
    "            - input.shape = (m, n)  # (batch_size, n_features)\n",
    "            - W.shape = (n, o)      # (n_features, n_neurons)\n",
    "            - b.shape = (o,)        # (n_neurons,) -> broadcasted to (m, o)\n",
    "            - output.shape = (m, o) # (batch_size, n_neurons)\n",
    "\n",
    "        Parameters:\n",
    "            - n_features (int): The number of input features\n",
    "            - n_neurons (int): The number of neurons in this layer\n",
    "        \"\"\"\n",
    "        # He initialization for weights and zeros for biases\n",
    "        self.params = {\n",
    "            'W': np.random.randn(n_features, n_neurons) * np.sqrt(2. / n_features),\n",
    "            'b': np.zeros(n_neurons)\n",
    "        }\n",
    "        # Initialize gradients as empty dictionaries\n",
    "        self.grads = {\n",
    "            'dW': np.zeros_like(self.params['W']),\n",
    "            'db': np.zeros_like(self.params['b'])\n",
    "        }\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the linear layer.\n",
    "\n",
    "        Parameters:\n",
    "            - input (np.array): The input array to the layer\n",
    "\n",
    "        Returns:\n",
    "            - output (np.array): The result of the linear transformation\n",
    "        \"\"\"\n",
    "        self.X = input  # Save the input for backward pass\n",
    "        W = self.params['W']\n",
    "        b = self.params['b']\n",
    "        # Linear transformation\n",
    "        output = np.dot(self.X, W) + b\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Performs the backward pass and calculates the gradients.\n",
    "\n",
    "        Parameters:\n",
    "            - dout (np.array): The gradient of the next layer (n + 1)\n",
    "\n",
    "        Returns:\n",
    "            - dx (np.array): The gradient with respect to the input (n - 1)\n",
    "        \"\"\"\n",
    "        # Calculate gradients\n",
    "        self.grads['dW'] = np.dot(self.X.T, dout)  # Gradient of weights\n",
    "        self.grads['db'] = np.sum(dout, axis=0)    # Gradient of biases\n",
    "        # Gradient with respect to the input to propagate to the previous layer\n",
    "        dx = np.dot(dout, self.params['W'].T)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Activation layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Module):  \n",
    "    def __init__(self, activation_name): \n",
    "        \"\"\"\n",
    "        Use to select activation function.\n",
    "        \n",
    "        Parameters:\n",
    "            - activation_name (str): name of activation functi\"o\"n.\n",
    "              Supported list: ['relu', 'leakyrelu', 'tanh', 'sigmoid', 'softmax']\n",
    "        \n",
    "        Return: \n",
    "            - None (apply activation function selected)\n",
    "        \"\"\"\n",
    "        self.activation_functions = {\n",
    "            'tanh': (self.tanh, self.tanh_prime), \n",
    "            'sigmoid': (self.sigmoid, self.sigmoid_prime), \n",
    "            'relu': (self.relu, self.relu_prime), \n",
    "            'leakyrelu': (self.leakyrelu, self.leakyrelu_prime), \n",
    "            'softmax': (self.softmax, self.softmax_prime)\n",
    "        }\n",
    "\n",
    "        self.activation_name = activation_name.lower() \n",
    "        if self.activation_name not in self.activation_functions: \n",
    "            raise ValueError(f'Activation function {self.activation_name} is not supported. Please choose from {list(self.activation_functions.keys())}')\n",
    "\n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "        Apply activation function.\n",
    "        \n",
    "        Parameters: \n",
    "            - x (np.array): input array \n",
    "        \n",
    "        Return: \n",
    "            - result (np.array) after applying the activation function.\n",
    "        \"\"\"\n",
    "        self.x = x \n",
    "        activation_func, _ = self.activation_functions[self.activation_name]\n",
    "        return activation_func(self.x) \n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        \"\"\"\n",
    "        Compute gradient for the previous layer.\n",
    "        \n",
    "        Parameters: \n",
    "            - output_gradient (np.array): the gradient of the later layer.\n",
    "        \n",
    "        Return: \n",
    "            - input_gradient (np.array): gradient for the previous layer.\n",
    "        \"\"\"\n",
    "        if self.activation_name == 'softmax': \n",
    "            # For softmax, usually combined with categorical cross-entropy, just return the output gradient\n",
    "            return output_gradient\n",
    "        else:\n",
    "            _, gradient = self.activation_functions[self.activation_name]\n",
    "            return output_gradient * gradient(self.x) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x): \n",
    "        x = np.clip(x, -500, 500) \n",
    "        return 1 / (1 + np.exp(-x)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_prime(x): \n",
    "        s = Activation.sigmoid(x)  \n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x): \n",
    "        return np.tanh(x) \n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_prime(x): \n",
    "        return 1 - np.tanh(x) ** 2 \n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x): \n",
    "        return np.maximum(0, x) \n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_prime(x): \n",
    "        return np.where(x > 0, 1, 0) \n",
    "\n",
    "    @staticmethod\n",
    "    def leakyrelu(x, alpha=0.1):\n",
    "        return np.where(x > 0, x, alpha * x) \n",
    "    \n",
    "    @staticmethod\n",
    "    def leakyrelu_prime(x, alpha=0.1):\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x): \n",
    "        # Stable softmax implementation\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return output \n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_prime(softmax_output): \n",
    "        \"\"\"\n",
    "        The derivative of softmax is usually not implemented separately because it is rarely used\n",
    "        directly. When combined with cross-entropy loss, the gradient simplifies to just the \n",
    "        output gradient from the loss function, making it computationally eficient.\n",
    "        \n",
    "        If needed, the derivative involves computing a Jacobian matrix which can be complex \n",
    "        and is generally avoided in practice.\n",
    "        \n",
    "        Raise NotImplementedError to indicate its limited practical usage.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"The derivative of softmax is rarely used directly. In practice, \"\n",
    "            \"softmax is combined with cross-entropy loss, which simplifies the \"\n",
    "            \"gradient computation during backpropagation. If you need the exact \"\n",
    "            \"Jacobian matrix, consider implementing a specialized function.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO ADJUST THE LOSS CLASSS\n",
    "\n",
    "class Loss(Module):    \n",
    "    def __init__(self, loss_name, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Choose loss function from the supported list.\n",
    "        Parameters:\n",
    "            - loss_name (str): Name of the loss function. Options: [mse, binary_cross_entropy, categorical_cross_entropy]\n",
    "            - reduction (str): Reduction type: [mean, sum, none]\n",
    "        \n",
    "        Return:\n",
    "            - None\n",
    "        \"\"\"\n",
    "        self.losses_func = {\n",
    "            'mse': (self.mse, self.mse_prime),\n",
    "            'binary_cross_entropy': (self.binary_cross_entropy, self.binary_cross_entropy_prime),\n",
    "            'categorical_cross_entropy': (self.categorical_cross_entropy, self.categorical_cross_entropy_prime),\n",
    "        }\n",
    "\n",
    "        self.loss_name = loss_name.lower()\n",
    "        self.reduction = reduction.lower()\n",
    "        if self.loss_name not in self.losses_func:\n",
    "            raise ValueError(f'Loss function {self.loss_name} is not supported. Choose from {list(self.losses_func.keys())}')\n",
    "\n",
    "    def __apply_reduction(self, loss):\n",
    "        if self.reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise ValueError(f'The reduction method {self.reduction} is not supported')\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Apply loss function.\n",
    "        Parameters:\n",
    "            - y_true (np.array): True values\n",
    "            - y_pred (np.array): Predicted values\n",
    "        \n",
    "        Return:\n",
    "            - Loss value after reduction\n",
    "        \"\"\"\n",
    "        loss_function, _ = self.losses_func[self.loss_name]\n",
    "        loss = loss_function(y_true, y_pred)\n",
    "        return self.__apply_reduction(loss)\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradient of the loss.\n",
    "        Parameters:\n",
    "            - y_true (np.array): True values\n",
    "            - y_pred (np.array): Predicted values\n",
    "        \n",
    "        Return:\n",
    "            - Gradient for the previous layer\n",
    "        \"\"\"\n",
    "        _, gradient = self.losses_func[self.loss_name]\n",
    "        grad = gradient(y_true, y_pred)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "        return (y_true - y_pred) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_prime(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy_prime(y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "    @staticmethod  \n",
    "    def categorical_cross_entropy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy Loss:\n",
    "        Calculates the loss between the true labels and predicted probabilities.\n",
    "        Parameters:\n",
    "            - y_true (np.array): True one-hot encoded labels\n",
    "            - y_pred (np.array): Predicted probabilities\n",
    "        \n",
    "        Return:\n",
    "            - Computed categorical cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Clip y_pred to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        # Compute categorical cross-entropy\n",
    "        loss = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy_prime(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Gradient of Categorical Cross-Entropy Loss:\n",
    "        Calculates the gradient of the loss with respect to the predictions.\n",
    "        Parameters:\n",
    "            - y_true (np.array): True one-hot encoded labels\n",
    "            - y_pred (np.array): Predicted probabilities\n",
    "        \n",
    "        Return:\n",
    "            - Gradient of the loss\n",
    "        \"\"\"\n",
    "        # Gradient of categorical cross-entropy with respect to predictions\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        grad = (y_pred - y_true) / y_true.shape[0]\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError('The step method must be implemented by the subclass')\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, learning_rate=0.1, momentum=0.0):\n",
    "        super().__init__(learning_rate)\n",
    "        self.layers = model\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # Initialize velocities for all parameters as a dictionary of layer IDs\n",
    "        self.velocities = {id(layer): {param_name: 0 for param_name in layer.params} for layer in self.layers if hasattr(layer, 'params')}\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step, updating the provided parameters.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for param_name in layer.params:\n",
    "                    # Extract gradient\n",
    "                    dparam = layer.grads[f'd{param_name}']\n",
    "\n",
    "                    # Compute velocities\n",
    "                    self.velocities[id(layer)][param_name] = self.momentum * self.velocities[id(layer)][param_name] - self.lr * dparam\n",
    "\n",
    "                    # Update params together\n",
    "                    layer.params[param_name] += self.velocities[id(layer)][param_name]\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.layers = model\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step counter\n",
    "\n",
    "        # Initialize first (m) and second (v) moment estimates for all parameters\n",
    "        self.m = {id(layer): {param_name: 0 for param_name in layer.params} \n",
    "                  for layer in self.layers if hasattr(layer, 'params')}\n",
    "        self.v = {id(layer): {param_name: 0 for param_name in layer.params} \n",
    "                  for layer in self.layers if hasattr(layer, 'params')}\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step using the Adam algorithm.\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment time step\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for param_name in layer.params:\n",
    "                    # Extract gradient\n",
    "                    dparam = layer.grads[f'd{param_name}']\n",
    "\n",
    "                    # Update first moment estimate (m)\n",
    "                    self.m[id(layer)][param_name] = self.beta1 * self.m[id(layer)][param_name] + (1 - self.beta1) * dparam\n",
    "\n",
    "                    # Update second moment estimate (v)\n",
    "                    self.v[id(layer)][param_name] = self.beta2 * self.v[id(layer)][param_name] + (1 - self.beta2) * (dparam ** 2)\n",
    "\n",
    "                    # Bias correction for first moment (m_hat)\n",
    "                    m_hat = self.m[id(layer)][param_name] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "                    # Bias correction for second moment (v_hat)\n",
    "                    v_hat = self.v[id(layer)][param_name] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                    # Update parameters\n",
    "                    layer.params[param_name] -= self.lr * m_hat / (v_hat ** 0.5 + self.epsilon)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2]\n",
      "X.shape= (7200, 2)\n",
      "y.shape= (7200, 3)\n"
     ]
    }
   ],
   "source": [
    "import Points as points\n",
    "\n",
    "spiral = points.Circle(3000, 3, 2) \n",
    "X = spiral.P\n",
    "y = spiral.L\n",
    "X_train, y_train, X_test, y_test = data.train_test_split(inputs= X, labels=y, ratio=0.8)\n",
    "\n",
    "# Standard scaler \n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test) \n",
    "\n",
    "# Encoder \n",
    "encoder = OnehotEncoder() \n",
    "encoder.fit(y_train) \n",
    "y_train = encoder.transform()\n",
    "\n",
    "# Check out the shape \n",
    "print(f'X.shape= {X_train.shape}')\n",
    "print(f'y.shape= {y_train.shape}')\n",
    "\n",
    "# Create dataloader \n",
    "trainloader = data.DataLoader(X_train, y_train, 32, shuffle= True) \n",
    "testloader = data.DataLoader(X_test, y_test, 32, shuffle= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training error = 0.09122743618058475, Training accuracy: 96.43056%\n",
      "Epoch 200, Training error = 0.0887182564842566, Training accuracy: 96.55556%\n",
      "Epoch 300, Training error = 0.08725881127278104, Training accuracy: 96.59722%\n",
      "Epoch 400, Training error = 0.08560742965671357, Training accuracy: 96.59722%\n",
      "Epoch 500, Training error = 0.08416777719554552, Training accuracy: 96.61111%\n",
      "Epoch 600, Training error = 0.08283073622665714, Training accuracy: 96.66667%\n",
      "Epoch 700, Training error = 0.08182449318279744, Training accuracy: 96.77778%\n",
      "Epoch 800, Training error = 0.08033521274892087, Training accuracy: 96.65278%\n",
      "Epoch 900, Training error = 0.079252967709406, Training accuracy: 96.58333%\n",
      "Epoch 1000, Training error = 0.07866039249480161, Training accuracy: 96.62500%\n"
     ]
    }
   ],
   "source": [
    "# Define network \n",
    "model = Sequential([\n",
    "    Linear(2, 32),\n",
    "    Activation('relu'), \n",
    "    Linear(32, 64),\n",
    "    Activation('relu'), \n",
    "    Linear(64, 3), \n",
    "    Activation('softmax') \n",
    "])\n",
    "\n",
    "# Define loss and optimizer \n",
    "loss = Loss(loss_name= 'categorical_cross_entropy')\n",
    "optimizer  =Adam(model, learning_rate= 0.001) \n",
    "\n",
    "# Train loop \n",
    "verbose = True\n",
    "NUM_EPOCHS = 1000 \n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    training_error = 0.0 \n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    for x_train, y_train in trainloader: \n",
    "        # Forward \n",
    "        y_pred = model.forward(x_train)\n",
    "\n",
    "        # Loss \n",
    "        training_error += loss.forward(y_train, y_pred) \n",
    "\n",
    "        # Backward \n",
    "        dout = loss.backward(y_train, y_pred) \n",
    "        model.backward(dout)\n",
    "        \n",
    "        # Calculate accuracy \n",
    "        predicted_classes = np.argmax(y_pred, axis= 1) \n",
    "        labels = np.argmax(y_train, axis= 1) \n",
    "\n",
    "        correct += (predicted_classes == labels).sum() \n",
    "        total += len(y_train)\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute + Display \n",
    "    training_error /= len(trainloader)\n",
    "    training_accuracy = 100*(correct / total )\n",
    "    if verbose and (epoch % 100) == 99:     \n",
    "        print(f'Epoch {epoch + 1}, Training error = {training_error}, Training accuracy: {training_accuracy:.5f}%')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9678\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for correct predictions and total samples\n",
    "correct = 0 \n",
    "total = 0 \n",
    "val_error = 0\n",
    "\n",
    "for x_test, y_test in testloader: \n",
    "    # Predict \n",
    "    y_pred = model.forward(x_test)\n",
    "\n",
    "    # Calculate predicted classes\n",
    "    predicted_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Count correct predictions\n",
    "    correct += (predicted_class == y_test).sum()\n",
    "    \n",
    "    # Update the total number of samples\n",
    "    total += len(y_test)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = correct / total\n",
    "print(f'Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 09:20:53.998872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-03 09:20:54.044183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-03 09:20:54.056881: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-03 09:20:54.136524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-03 09:20:55.135085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 784)\n",
      "Training labels shape: (60000,)\n",
      "Test data shape: (10000, 784)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the data\n",
    "x_train = x_train.reshape(-1, 28 * 28) / 255.0  # Flatten and normalize\n",
    "x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
    "\n",
    "# Convert the data to float32 for compatibility with most neural networks\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Training labels shape: {y_train.shape}')\n",
    "print(f'Test data shape: {x_test.shape}')\n",
    "print(f'Test labels shape: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "X.shape = (64, 784)\n",
      "y.shape = (64, 10)\n"
     ]
    }
   ],
   "source": [
    "# Standard scaler \n",
    "scaler = StandardScaler() \n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train) \n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Encoder \n",
    "encoder = OnehotEncoder()\n",
    "encoder.fit(y_train) \n",
    "y_train = encoder.transform()\n",
    "\n",
    "# Dataloader \n",
    "trainloader = data.DataLoader(x_train, y_train, 64, True) \n",
    "testloader = data.DataLoader(x_test, y_test, 64, False) \n",
    "\n",
    "# Check \n",
    "dataiter = iter(trainloader) \n",
    "features, labels = next(dataiter)\n",
    "print(f'X.shape = {features.shape}')\n",
    "print(f'y.shape = {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Error = 0.3202, Training Accuracy = 90.69%\n",
      "Epoch 2, Error = 0.1234, Training Accuracy = 96.36%\n",
      "Epoch 3, Error = 0.0793, Training Accuracy = 97.67%\n",
      "Epoch 4, Error = 0.0544, Training Accuracy = 98.46%\n",
      "Epoch 5, Error = 0.0380, Training Accuracy = 99.01%\n",
      "Epoch 6, Error = 0.0270, Training Accuracy = 99.37%\n",
      "Epoch 7, Error = 0.0181, Training Accuracy = 99.62%\n",
      "Epoch 8, Error = 0.0134, Training Accuracy = 99.74%\n",
      "Epoch 9, Error = 0.0146, Training Accuracy = 99.63%\n",
      "Epoch 10, Error = 0.0175, Training Accuracy = 99.53%\n"
     ]
    }
   ],
   "source": [
    "# Define network \n",
    "model = Sequential([\n",
    "    Linear(784, 128),\n",
    "    Activation('relu'), \n",
    "    Linear(128, 64),\n",
    "    Activation('relu'), \n",
    "    Linear(64, 10), \n",
    "    Activation('softmax') \n",
    "])\n",
    "\n",
    "# Define loss and optimizer \n",
    "loss = Loss(loss_name='categorical_cross_entropy')\n",
    "optimizer = Adam(model, learning_rate=0.0005) \n",
    "\n",
    "# Train loop \n",
    "verbose = True \n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    error = 0 \n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x_train, y_train in trainloader: \n",
    "        # Forward \n",
    "        y_pred = model.forward(x_train)\n",
    "\n",
    "        # Loss \n",
    "        error += loss.forward(y_train, y_pred) \n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_train, axis=1)\n",
    "        correct_predictions += np.sum(predicted_labels == true_labels)\n",
    "        total_samples += y_train.shape[0]\n",
    "\n",
    "        # Backward \n",
    "        dout = loss.backward(y_train, y_pred) \n",
    "        model.backward(dout)\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average error and accuracy\n",
    "    error /= len(trainloader)\n",
    "    training_accuracy = correct_predictions / total_samples * 100  # Percentage\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Epoch {epoch + 1}, Error = {error:.4f}, Training Accuracy = {training_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.04 %\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for correct predictions and total samples\n",
    "correct = 0 \n",
    "total = 0 \n",
    "val_error = 0\n",
    "\n",
    "for x_test, y_test in testloader: \n",
    "    # Predict \n",
    "    y_pred = model.forward(x_test)\n",
    "\n",
    "    # Calculate predicted classes\n",
    "    predicted_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Count correct predictions\n",
    "    correct += (predicted_class == y_test).sum()\n",
    "    \n",
    "    # Update the total number of samples\n",
    "    total += len(y_test)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = ( correct / total ) * 100 \n",
    "print(f'Accuracy: {acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
